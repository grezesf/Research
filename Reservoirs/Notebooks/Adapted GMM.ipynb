{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from the UBM, build adapted models of the phones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# general imports\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import Scykit learn GMM library\n",
    "from sklearn import mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['find_phone_index', 'load_phone_file']\n"
     ]
    }
   ],
   "source": [
    "# import custom functions\n",
    "import sys\n",
    "# path to libraries\n",
    "# currently in ../scripts-lib/\n",
    "tool_path = os.path.abspath('../scripts-lib')\n",
    "\n",
    "if tool_path not in sys.path:\n",
    "    sys.path.append(tool_path)\n",
    "import lib_phones as lph\n",
    "\n",
    "# print the loaded functions\n",
    "print dir(lph)[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 ['aa', 'ae', 'ah', 'ao', 'aw', 'ax', 'ax-h', 'axr', 'ay', 'b', 'bcl', 'ch', 'd', 'dcl', 'dh', 'dx', 'eh', 'el', 'em', 'en', 'eng', 'epi', 'er', 'ey', 'f', 'g', 'gcl', 'h#', 'hh', 'hv', 'ih', 'ix', 'iy', 'jh', 'k', 'kcl', 'l', 'm', 'n', 'ng', 'nx', 'ow', 'oy', 'p', 'pau', 'pcl', 'q', 'r', 's', 'sh', 't', 'tcl', 'th', 'uh', 'uw', 'ux', 'v', 'w', 'y', 'z', 'zh']\n"
     ]
    }
   ],
   "source": [
    "# load phone list\n",
    "phone_path = os.path.abspath('../datasets/TIMIT-MFCCs/TIMIT_phone_list.txt')\n",
    "phone_list = lph.load_phone_file(phone_path)\n",
    "print len(phone_list), phone_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working in path : C:\\Users\\FG\\Desktop\\PhD\\Research\\Reservoirs\\datasets\\TIMIT-MFCCs\\dev\n",
      "working on: si1027.mfcc.csv\n",
      "working on: si1105.mfcc.csv\n",
      "working on: si1657.mfcc.csv\n",
      "working on: si1735.mfcc.csv\n",
      "working on: si475.mfcc.csv\n",
      "working on: si648.mfcc.csv\n",
      "working on: sx115.mfcc.csv\n",
      "working on: sx127.mfcc.csv\n",
      "working on: sx205.mfcc.csv\n",
      "working on: sx217.mfcc.csv\n",
      "working on: sx25.mfcc.csv\n",
      "working on: sx295.mfcc.csv\n",
      "working on: sx307.mfcc.csv\n",
      "working on: sx37.mfcc.csv\n",
      "working on: sx385.mfcc.csv\n",
      "working on: sx397.mfcc.csv\n"
     ]
    }
   ],
   "source": [
    "#load mfccs into sklearn observations, each frame is an obs\n",
    "\n",
    "train_TIMIT_dir = os.path.abspath('../datasets/TIMIT-MFCCs/dev')\n",
    "\n",
    "#create individual obs list for each phone type\n",
    "train_phone_obs_dict = {}\n",
    "for phone in phone_list:\n",
    "    train_phone_obs_dict[phone] = []\n",
    "\n",
    "# complete obs \n",
    "train_obs = []\n",
    "train_obs_labels = []\n",
    "\n",
    "# walk the directories\n",
    "for (path, dirs, files) in os.walk(train_TIMIT_dir):\n",
    "    print \"working in path : \" + path\n",
    "\n",
    "    for file in files:\n",
    "        # skip the SA files\n",
    "        #dev, only work on file si1573.mfcc.csv     \"si1573\" in file and\n",
    "        if \".mfcc\" in file  and \"sa\" not in file:\n",
    "            #check if corresponding .phn file exists\n",
    "            if not os.path.exists(path + \"/\" + file[:-8] + \"phn\"):\n",
    "                print path + \"/\" + file[:-8] + \"phn\"\n",
    "                print \"corresponding .phn file does not exist!\"\n",
    "            else:\n",
    "                \n",
    "                print \"working on: \" + file\n",
    "#                 print \"from path : \" + path\n",
    "\n",
    "                # open the files\n",
    "                mfcc_file = open(path + \"/\" + file)\n",
    "                phn_file = open(path + \"/\" + file[:-8] + \"phn\")\n",
    "\n",
    "                # extract phone times\n",
    "                phone_times = []\n",
    "                for phn_line in phn_file:\n",
    "                    phone_times.append(phn_line.split())\n",
    "                # transpose for easier use\n",
    "                phone_times = map(list, zip(*phone_times))\n",
    "\n",
    "                # skip mfcc_file header\n",
    "                next(mfcc_file)\n",
    "\n",
    "                # reset frame count\n",
    "                frame_cnt = 0\n",
    "\n",
    "                # for each line of mfcc_file\n",
    "                for mfcc_line in mfcc_file:\n",
    "\n",
    "                    # increment frame count\n",
    "                    frame_cnt += 1 \n",
    "\n",
    "                    # print \"frame line #:\", frame_cnt \n",
    "\n",
    "                    # frame start time in seconds\n",
    "                    start_t = mfcc_line.split(\";\")[1]\n",
    "\n",
    "                    # create frame (skiping first 2 values, frame_index and frame_time)\n",
    "                    frame = map( float,  mfcc_line.split(\";\")[2:])\n",
    "                    # print numpy.shape(frame)\n",
    "                    # print frame\n",
    "\n",
    "                    # find correspond phoneme and index in the list\n",
    "                    phn_index = lph.find_phone_index(start_t, phone_times, phone_list)\n",
    "\n",
    "                    # add to instances for corresponding \n",
    "                    train_phone_obs_dict[phone_list[phn_index]].append(frame)\n",
    "                    # add to instances\n",
    "                    train_obs.append(frame)\n",
    "                    train_obs_labels.append(phone_list[phn_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 482\n"
     ]
    }
   ],
   "source": [
    "print len(train_phone_obs_dict), len(train_phone_obs_dict[\"h#\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded UBM gmm from  C:\\Users\\FG\\Desktop\\PhD\\Research\\Reservoirs\\datasets\\TIMIT Pickled Data\\TIMIT_ubm_gmm_10.pckl\n"
     ]
    }
   ],
   "source": [
    "# reload pickled UBM file\n",
    "\n",
    "pickle_dir = os.path.abspath('../datasets/TIMIT Pickled Data')\n",
    "\n",
    "# find name\n",
    "for file in os.listdir(pickle_dir):\n",
    "    if \"TIMIT\" in file and 'ubm' in file and \".pckl\" in file:\n",
    "        pickle_name = file\n",
    "\n",
    "ubm = pickle.load( open(pickle_dir + os.sep + pickle_name, \"rb\") )\n",
    "print \"loaded UBM gmm from \", pickle_dir + os.sep + pickle_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # print some stats about the UBM\n",
    "# print np.round(ubm.score(train_obs).mean(), 2)\n",
    "# print np.round(ubm.score(train_obs).var(), 2)\n",
    "# print np.round(ubm.score(train_obs)[0:5] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "not enough obs for phone ch\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "not enough obs for phone en\n",
      "10\n",
      "not enough obs for phone eng\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "not enough obs for phone nx\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "not enough obs for phone uh\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "not enough obs for phone y\n",
      "10\n",
      "10\n",
      "not enough obs for phone zh\n"
     ]
    }
   ],
   "source": [
    "# dictionary containing a gmm for each phone, adapted from the UBM\n",
    "agmm_dict = dict()\n",
    "\n",
    "# adapt UBM to the phone obs\n",
    "for phone in phone_list:\n",
    "    # make copy of gmm\n",
    "    agmm = copy.copy(ubm)\n",
    "    \n",
    "    if agmm.n_components > len(train_phone_obs_dict[phone]):\n",
    "        # no enough observations\n",
    "        print \"not enough obs for phone\", phone\n",
    "        pass\n",
    "    else:\n",
    "        # adapt\n",
    "        agmm.fit(train_phone_obs_dict[phone])\n",
    "        # add to dictionary\n",
    "        agmm_dict[phone] = agmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved adapted gmm dictionary in  C:\\Users\\FG\\Desktop\\PhD\\Research\\Reservoirs\\datasets\\TIMIT Pickled Data\\TIMIT_gmm_adapted_dict.pckl\n"
     ]
    }
   ],
   "source": [
    "#save gmm in pickled form\n",
    "\n",
    "# name and location to save in\n",
    "pickle_name = \"TIMIT_gmm_adapted_dict\" + \".pckl\"\n",
    "pickle_dir = os.path.abspath('..'+ os.sep + 'datasets' + os.sep + 'TIMIT Pickled Data')\n",
    "\n",
    "if not os.path.isdir(pickle_dir):\n",
    "    os.makedirs(pickle_dir)\n",
    "    \n",
    "pickle.dump( agmm_dict, open( pickle_dir + os.sep + pickle_name, \"wb\") )\n",
    "print \"saved adapted gmm dictionary in \", pickle_dir + os.sep + pickle_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
